{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADHD Classification Based on HRV\n",
    "# Purpose: Determine whether heart rate variability can predict diagnosis of ADHD\n",
    "# Author: Alexander Maksiaev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/maksi/Documents/Statistics/Projects/Movement_Mental_Health/hyperaktiv/hyperaktiv_with_controls/hyperaktiv_with_controls\n",
      "C:/Users/maksi/Documents/Statistics/Projects/Movement_Mental_Health/hyperaktiv/hyperaktiv_with_controls/hyperaktiv_with_controls\\hrv_data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACC_TIME</th>\n",
       "      <th>ACC_DAYS</th>\n",
       "      <th>HRV</th>\n",
       "      <th>HRV_TIME</th>\n",
       "      <th>HRV_HOURS</th>\n",
       "      <th>CPT_II</th>\n",
       "      <th>...</th>\n",
       "      <th>HADS_D</th>\n",
       "      <th>MED</th>\n",
       "      <th>MED_Antidepr</th>\n",
       "      <th>MED_Moodstab</th>\n",
       "      <th>MED_Antipsych</th>\n",
       "      <th>MED_Anxiety_Benzo</th>\n",
       "      <th>MED_Sleep</th>\n",
       "      <th>MED_Analgesics_Opioids</th>\n",
       "      <th>MED_Stimulants</th>\n",
       "      <th>filter_$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>11:00:00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10:54:00</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15:28:00</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1</td>\n",
       "      <td>15:25:00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>16:55:00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14:24:00</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9:30:00</td>\n",
       "      <td>13,3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9:00:00</td>\n",
       "      <td>14,6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9:00:00</td>\n",
       "      <td>14,3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9:00:00</td>\n",
       "      <td>14,6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  SEX  AGE  ACC  ACC_TIME ACC_DAYS  HRV  HRV_TIME  HRV_HOURS  CPT_II  \\\n",
       "0     01    0    3    1  16:00:00        6    1  11:00:00       21.0       0   \n",
       "1     02    0    4    1  10:54:00      6.8    0       NaN        NaN       1   \n",
       "2     03    1    2    1  15:28:00      7.2    1  15:25:00       21.0       1   \n",
       "3     04    1    3    0       NaN      NaN    1  16:55:00       22.0       1   \n",
       "4     05    1    1    1  14:24:00      5.9    1  16:00:00       12.0       1   \n",
       "..   ...  ...  ...  ...       ...      ...  ...       ...        ...     ...   \n",
       "129  236    1    2    1   9:30:00     13,3    0       NaN        NaN       0   \n",
       "130  237    0    1    1  15:00:00       14    0       NaN        NaN       0   \n",
       "131  238    0    4    1   9:00:00     14,6    0       NaN        NaN       0   \n",
       "132  239    0    4    1   9:00:00     14,3    0       NaN        NaN       0   \n",
       "133  240    1    1    1   9:00:00     14,6    0       NaN        NaN       0   \n",
       "\n",
       "     ...  HADS_D  MED  MED_Antidepr  MED_Moodstab  MED_Antipsych  \\\n",
       "0    ...     2.0  1.0           1.0           NaN            NaN   \n",
       "1    ...     7.0  0.0           NaN           NaN            NaN   \n",
       "2    ...     0.0  0.0           NaN           NaN            NaN   \n",
       "3    ...     6.0  1.0           1.0           NaN            NaN   \n",
       "4    ...     5.0  0.0           NaN           NaN            NaN   \n",
       "..   ...     ...  ...           ...           ...            ...   \n",
       "129  ...     NaN  NaN           NaN           NaN            NaN   \n",
       "130  ...     NaN  NaN           NaN           NaN            NaN   \n",
       "131  ...     NaN  NaN           NaN           NaN            NaN   \n",
       "132  ...     NaN  NaN           NaN           NaN            NaN   \n",
       "133  ...     NaN  NaN           NaN           NaN            NaN   \n",
       "\n",
       "     MED_Anxiety_Benzo  MED_Sleep  MED_Analgesics_Opioids  MED_Stimulants  \\\n",
       "0                  NaN        NaN                     NaN             NaN   \n",
       "1                  NaN        NaN                     NaN             NaN   \n",
       "2                  NaN        NaN                     NaN             NaN   \n",
       "3                  1.0        NaN                     1.0             NaN   \n",
       "4                  NaN        NaN                     NaN             NaN   \n",
       "..                 ...        ...                     ...             ...   \n",
       "129                NaN        NaN                     NaN             NaN   \n",
       "130                NaN        NaN                     NaN             NaN   \n",
       "131                NaN        NaN                     NaN             NaN   \n",
       "132                NaN        NaN                     NaN             NaN   \n",
       "133                NaN        NaN                     NaN             NaN   \n",
       "\n",
       "     filter_$  \n",
       "0         1.0  \n",
       "1         1.0  \n",
       "2         1.0  \n",
       "3         0.0  \n",
       "4         1.0  \n",
       "..        ...  \n",
       "129       NaN  \n",
       "130       NaN  \n",
       "131       NaN  \n",
       "132       NaN  \n",
       "133       NaN  \n",
       "\n",
       "[134 rows x 33 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Housekeeping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sktime.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tsfresh import extract_features\n",
    "# from tsfresh import extract_relevant_features\n",
    "# from tsfresh import select_features\n",
    "# import featuretools as ft \n",
    "# from featuretools.selection import (\n",
    "#     remove_highly_correlated_features,\n",
    "#     remove_highly_null_features,\n",
    "#     remove_single_value_features,\n",
    "# )\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.pipeline import make_pipeline\n",
    "from sktime.transformations.panel.catch22 import Catch22\n",
    "from sklearn.metrics import accuracy_score\n",
    "import glob\n",
    "import os\n",
    "import csv \n",
    "from datetime import datetime\n",
    "\n",
    "# Directories\n",
    "\n",
    "adhd_dir_raw = \"C:/Users\\maksi\\Documents\\Statistics\\Projects\\Movement_Mental_Health\\hyperaktiv\\hyperaktiv_with_controls\\hyperaktiv_with_controls\"\n",
    "adhd_dir = adhd_dir_raw.replace(\"\\\\\", \"/\")\n",
    "print(adhd_dir)\n",
    "activity_dir = os.path.join(adhd_dir, \"activity_data\")\n",
    "hrv_dir = os.path.join(adhd_dir, \"hrv_data\")\n",
    "print(hrv_dir)\n",
    "output_dir = os.path.join(adhd_dir, \"pt_features\") \n",
    "# controls_dir = os.path.join(adhd_dir, \"hyperaktiv_with_controls/hyperaktiv_with_controls/\")\n",
    "# activity_dir_controls = os.path.join(controls_dir, \"activity_data/\")\n",
    "\n",
    "os.chdir(adhd_dir)\n",
    "\n",
    "# Main files\n",
    "patient_info = pd.read_csv(\"patient_info.csv\", delimiter=\";\")\n",
    "# features = pd.read_csv(\"features.csv\", delimiter=\";\")\n",
    "patient_info['ID'] = patient_info['ID'].astype(\"string\").str.zfill(2)\n",
    "patient_info\n",
    "# print(features)\n",
    "\n",
    "# os.chdir(activity_dir)\n",
    "# activity_data_01 = pd.read_csv(\"patient_activity_01.csv\", delimiter=\";\")\n",
    "# activity_data_01[\"ID\"] = 1\n",
    "# activity_data_01\n",
    "\n",
    "# os.chdir(hrv_dir)\n",
    "# hrv_data = pd.read_csv(\"patient_hr_1.csv\", delimiter=\";\")\n",
    "# hrv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:/Users/maksi/Documents/Statistics/Projects/Movement_Mental_Health/hyperaktiv/hyperaktiv_with_controls/hyperaktiv_with_controls\\activity_data\\patient_activity_01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction:   0%|          | 0/1 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "acf() got an unexpected keyword argument 'unbiased'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     data \u001b[38;5;241m=\u001b[39m read_activity_file(filepath, patient_id)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# data[\"TIME\"] = pd.to_datetime(data[\"TIME\"])\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# adhd_or_no = patient_info[patient_info[\"ID\"] == patient_id][\"ADHD\"].iloc[0]\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# data[\"ADHD\"] = adhd_or_no\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mACC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTIME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     features\u001b[38;5;241m.\u001b[39mto_csv(feature_filepath, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# data_transposed = data.T\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# data.index = data[\"TIME\"]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# tuples = list(zip(*arrays))\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# data_multi = pd.MultiIndex.from_product((patient_id), names=[\"ID\"]) # , \"TIME\", \"ACC\"])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py:151\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(timeseries_container, default_fc_parameters, kind_to_fc_parameters, column_id, column_sort, column_kind, column_value, chunksize, n_jobs, show_warnings, disable_progressbar, impute_function, profile, profiling_filename, profiling_sorting, distributor, pivot)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_do_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeseries_container\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdisable_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_warnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdistributor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpivot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Impute the result if requested\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m impute_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py:253\u001b[0m, in \u001b[0;36m_do_extraction\u001b[1;34m(df, column_id, column_value, column_kind, column_sort, default_fc_parameters, kind_to_fc_parameters, n_jobs, chunk_size, disable_progressbar, show_warnings, distributor, pivot)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe passed distributor is not an DistributorBaseClass object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(default_fc_parameters\u001b[38;5;241m=\u001b[39mdefault_fc_parameters,\n\u001b[0;32m    251\u001b[0m               kind_to_fc_parameters\u001b[38;5;241m=\u001b[39mkind_to_fc_parameters)\n\u001b[1;32m--> 253\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdistributor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_do_extraction_on_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mfunction_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pivot:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tsfresh\\utilities\\distribution.py:213\u001b[0m, in \u001b[0;36mIterableDistributorBaseClass.map_reduce\u001b[1;34m(self, map_function, data, function_kwargs, chunk_size, data_length)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute(_function_with_partly_reduce, chunk_generator, map_kwargs),\n\u001b[1;32m--> 213\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tsfresh\\utilities\\distribution.py:43\u001b[0m, in \u001b[0;36m_function_with_partly_reduce\u001b[1;34m(chunk_list, map_function, kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m     42\u001b[0m results \u001b[38;5;241m=\u001b[39m (map_function(chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_list)\n\u001b[1;32m---> 43\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tsfresh\\utilities\\distribution.py:42\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03mSmall helper function to call a function (map_function)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03mon a list of data chunks (chunk_list) and convert the results into\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m:rtype: list\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m---> 42\u001b[0m results \u001b[38;5;241m=\u001b[39m (\u001b[43mmap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_list)\n\u001b[0;32m     43\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(results))\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py:328\u001b[0m, in \u001b[0;36m_do_extraction_on_chunk\u001b[1;34m(chunk, default_fc_parameters, kind_to_fc_parameters)\u001b[0m\n\u001b[0;32m    325\u001b[0m                 feature_name \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(key)\n\u001b[0;32m    326\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m (sample_id, feature_name, item)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py:314\u001b[0m, in \u001b[0;36m_do_extraction_on_chunk.<locals>._f\u001b[1;34m()\u001b[0m\n\u001b[0;32m    311\u001b[0m     x \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39mfctype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombiner\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 314\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parameter_list:\n",
      "File \u001b[1;32mc:\\Users\\maksi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tsfresh\\feature_extraction\\feature_calculators.py:400\u001b[0m, in \u001b[0;36magg_autocorrelation\u001b[1;34m(x, param)\u001b[0m\n\u001b[0;32m    398\u001b[0m     a \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 400\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43macf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munbiased\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTHRESHOLD_TO_USE_FFT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_maxlag\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_agg_\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m__maxlag_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_agg\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxlag\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m    402\u001b[0m          \u001b[38;5;28mgetattr\u001b[39m(np, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_agg\u001b[39m\u001b[38;5;124m\"\u001b[39m])(a[:\u001b[38;5;28mint\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxlag\u001b[39m\u001b[38;5;124m\"\u001b[39m])])) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m param]\n",
      "\u001b[1;31mTypeError\u001b[0m: acf() got an unexpected keyword argument 'unbiased'"
     ]
    }
   ],
   "source": [
    "# Activity data feature extraction using sktime \n",
    "\n",
    "def read_activity_file(filepath, patient_id):\n",
    "    data = [ ]\n",
    "    with open(filepath) as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\";\")\n",
    "        next(csv_reader)\n",
    "        for line in csv_reader:\n",
    "            data.append([ datetime.strptime(line[0], \"%m-%d-%Y %H:%M\").timestamp(), int(line[1].split(\" \")[0])])\n",
    "    data = pd.DataFrame(data, columns=[\"TIME\", \"ACC\"])\n",
    "    data[\"ID\"] = patient_id\n",
    "    return data\n",
    "\n",
    "\n",
    "all_participants = pd.DataFrame()\n",
    "for filepath in glob.glob(os.path.join(activity_dir, \"*.csv\")):\n",
    "    print(\"Reading %s\" % filepath)\n",
    "    patient_id_raw = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    patient_id = patient_id_raw.split(\"_\")[-1]\n",
    "    feature_filepath = os.path.join(output_dir, \"%s_features.csv\" % patient_id)\n",
    "\n",
    "    if os.path.exists(feature_filepath):\n",
    "        print(\"Skipped...\")\n",
    "        continue\n",
    "    \n",
    "    data = read_activity_file(filepath, patient_id)\n",
    "\n",
    "    \n",
    "\n",
    "    # data[\"TIME\"] = pd.to_datetime(data[\"TIME\"])\n",
    "    # adhd_or_no = patient_info[patient_info[\"ID\"] == patient_id][\"ADHD\"].iloc[0]\n",
    "    # data[\"ADHD\"] = adhd_or_no\n",
    "    # features = extract_features(data, column_id=\"ID\", column_value=\"ACC\", column_sort=\"TIME\", n_jobs=0, show_warnings=False)\n",
    "    # features.to_csv(feature_filepath, index=False, sep=\";\")    \n",
    "\n",
    "    # data_transposed = data.T\n",
    "    \n",
    "    # data.index = data[\"TIME\"]\n",
    "    # # data.index.name = \"TIME\"\n",
    "    # # new_data = data.loc[:, [\"ACC\", \"ADHD\"]]\n",
    "    # data_multi = pd.MultiIndex.from_product([[patient_id], data[\"TIME\"]], names=[\"instances\", \"timepoints\"]) \n",
    "    # data.index = data_multi\n",
    "    # # print(data)\n",
    "    # data = sktime.datatypes.convert_to(data.sort_index(), to_type=\"pd-multiindex\")\n",
    "\n",
    "    # catch22 = Catch22()\n",
    "    # randf = RandomForestClassifier(n_estimators=100)\n",
    "    # pipe = make_pipeline(catch22, randf)\n",
    "\n",
    "    # print(data[\"ACC\"].to_frame().shape)\n",
    "    # print(data[\"ADHD\"].to_frame().shape)\n",
    "\n",
    "    # print(data)\n",
    "\n",
    "    # # all_participants = pd.concat([all_participants, new_data])\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(data[\"ACC\"].to_frame(), data[\"ADHD\"].to_frame(), test_size=0.33, random_state=42)\n",
    "    \n",
    "    # print(X_train.sort_index().shape)\n",
    "    # print(y_train.sort_index().shape)\n",
    "\n",
    "    # print(sktime.datatypes.check_raise(X_train.sort_index(), mtype=\"pd-multiindex\"))\n",
    "    # # pipe.fit(X_train, y_train)\n",
    "    # pipe.fit(X_train.sort_index(), y_train.sort_index())\n",
    "    # y_pred = pipe.predict(X_test.sort_index())\n",
    "\n",
    "    # score = accuracy_score(y_test.sort_index(), y_pred)\n",
    "    # print(score)\n",
    "    # break\n",
    "\n",
    "# print(arrays)\n",
    "# tuples = list(zip(*arrays))\n",
    "# data_multi = pd.MultiIndex.from_product((patient_id), names=[\"ID\"]) # , \"TIME\", \"ACC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_participants.index = pd.MultiIndex.from_tuples(all_participants.index, names=[\"instances\", \"timepoints\"])\n",
    "# # print(all_participants)\n",
    "\n",
    "# # X is the data to classify, y is the ground truth \n",
    "\n",
    "# # diagnoses = patient_info[[\"ID\", \"ADHD\"]]\n",
    "# # # diagnoses_transposed = diagnoses.T\n",
    "# # # print(diagnoses_transposed)\n",
    "# # # y = pd.concat([all_participants, diagnoses])\n",
    "# # all_participants_index = all_participants.index\n",
    "# # all_participants = pd.merge(all_participants, diagnoses, left_on=\"instances\", right_on=\"ID\") # , right_on=\"ID\")\n",
    "# # y.index = all_participants_index\n",
    "# # print(all_participants)\n",
    "\n",
    "# all_participants = sktime.datatypes.convert_to(all_participants.sort_index(), to_type=\"pd-multiindex\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_participants)\n",
    "# print(sktime.datatypes.check_raise(all_participants, mtype=\"pd-multiindex\"))\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(all_participants, all_participants[\"ADHD\"], test_size=0.33, random_state=42)\n",
    "\n",
    "# catch22 = Catch22()\n",
    "# randf = RandomForestClassifier(n_estimators=100)\n",
    "# pipe = make_pipeline(catch22, randf)\n",
    "\n",
    "# print(y_train)\n",
    "# print(sktime.datatypes.check_raise(X_train.sort_index(), mtype=\"pd-multiindex\"))\n",
    "\n",
    "# # transformed_data_mv = catch22.fit_transform(X_train.sort_index())\n",
    "# # transformed_data_mv.head()\n",
    "\n",
    "# pipe.fit(X_train.sort_index(), y_train.sort_index())\n",
    "# y_pred = pipe.predict(X_test.sort_index())\n",
    "\n",
    "# accuracy_score(y_test.sort_index(), y_pred)\n",
    "\n",
    "# # features.to_csv(feature_filepath, index=False, sep=\";\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data initialization\n",
    "\n",
    "# os.chdir(adhd_dir)\n",
    "\n",
    "# controls_info = pd.read_csv(\"patient_info.csv\", delimiter=\";\") # Includes patients as well\n",
    "# # print(controls_info)\n",
    "\n",
    "# adhd_patients = controls_info[controls_info[\"ADHD\"] == 1.0] # ADHD only\n",
    "# # print(adhd_patients)\n",
    "# # print(controls_info.columns)\n",
    "\n",
    "# controls = controls_info[np.sum(controls_info.loc[:, \"ADHD\":\"OTHER\"], axis=1) == 0] # Neurotypicals only\n",
    "# non_adhd_controls = controls_info[controls_info[\"ADHD\"] == 0] # Non-ADHD only\n",
    "# # print(controls)\n",
    "# # print(non_adhd_controls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract HRV features for each participant\n",
    "\n",
    "# os.chdir(hrv_dir)\n",
    "\n",
    "# def read_hrv_file(filepath, patient_id):\n",
    "#     data = [ ]\n",
    "#     with open(filepath) as f:\n",
    "#         csv_reader = csv.reader(f, delimiter=\";\")\n",
    "#         next(csv_reader)\n",
    "#         for line in csv_reader:\n",
    "#             data.append([ datetime.strptime(line[0], \"%Y-%m-%d %H:%M:%S.%f\").timestamp(), float(line[1].split(\" \")[0])])\n",
    "#     data = pd.DataFrame(data, columns=[\"TIME\", \"HRV\"])\n",
    "#     data[\"ID\"] = patient_id\n",
    "#     return data\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# for filepath in glob.glob(os.path.join(hrv_dir, \"*.csv\")):\n",
    "#     print(\"Reading %s\" % filepath)\n",
    "#     patient_id = os.path.splitext(os.path.basename(filepath))[0]\n",
    "#     feature_filepath = os.path.join(output_dir, \"%s_features.csv\" % patient_id)\n",
    "\n",
    "#     if os.path.exists(feature_filepath):\n",
    "#         print(\"Skipped...\")\n",
    "#         continue\n",
    "    \n",
    "#     data = read_hrv_file(filepath, patient_id)\n",
    "\n",
    "#     es = ft.EntitySet(id=\"pt_data\")\n",
    "#     es = es.add_dataframe(\n",
    "#     dataframe_name=\"pt_data\",\n",
    "#     dataframe=data,\n",
    "#     )\n",
    "#     rolling_mean_primitive = RollingMean(\n",
    "#         window_length=window_length, gap=gap, min_periods=window_length\n",
    "#     )\n",
    "\n",
    "#     rolling_min_primitive = RollingMin(\n",
    "#         window_length=window_length, gap=gap, min_periods=window_length\n",
    "#     )\n",
    "#     feature_matrix, feature_defs = ft.dfs(\n",
    "#     entityset=es,\n",
    "#     target_dataframe_name=\"pt_data\",\n",
    "#     trans_primitives=(\n",
    "#         datetime_primitives\n",
    "#         + delaying_primitives\n",
    "#         + [rolling_mean_primitive, rolling_min_primitive]\n",
    "#     ),\n",
    "#     )\n",
    "#     print(feature_defs)\n",
    "#     print(feature_matrix)\n",
    "#     break \n",
    "#     # features.to_csv(feature_filepath, index=False, sep=\";\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine feature csvs into one file for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concat all activity dataframes\n",
    "\n",
    "# os.chdir(hrv_dir)\n",
    "\n",
    "# csv_files = glob.glob(\"*.csv\")\n",
    "\n",
    "# # Create an empty dataframe to store the combined data\n",
    "# # mega_activity_df = pd.DataFrame()\n",
    "\n",
    "# # Loop through each CSV file and append it to the combined dataframe\n",
    "# def get_activity_file(file):\n",
    "# # for file in csv_files:\n",
    "#     file_name_pieces = str.split(file, sep=\"_\")\n",
    "#     pt_id = file_name_pieces[-1][:-4] # Last part of the file name is patient id, without the .csv\n",
    "#     df = pd.read_csv(file, delimiter=\";\")\n",
    "#     df[\"ID\"] = int(pt_id)\n",
    "#     # mega_activity_df = pd.concat([mega_activity_df, df], ignore_index=True)\n",
    "#     return df\n",
    "\n",
    "# # mega_activity_df\n",
    "# # print(len(mega_activity_df))\n",
    "# # mega_activity_df = mega_activity_df.dropna()\n",
    "\n",
    "# # print(len(mega_activity_df[mega_activity_df[\"ID\"] <= 3]))\n",
    "# activity_01 = get_activity_file(csv_files[0])\n",
    "# print(activity_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: Activity\n",
    "\n",
    "# features = extract_features(activity_01, column_id=\"ID\", column_value=\"ACTIVITY\", column_sort=\"TIMESTAMP\", n_jobs=0, show_warnings=False)\n",
    "# print(features.head())\n",
    "# os.chdir(main_dir)\n",
    "# features.to_csv(\"adhd_activity_features.csv\", index=False, sep=\",\")    \n",
    "\n",
    "# # Pts with motor activity recordings\n",
    "# active_controls = controls_info[controls_info[\"ACC\"] == 1]\n",
    "# # print(active_controls)\n",
    "\n",
    "# # Each pt has their own csv file...\n",
    "# # demographics_01 = active_controls[active_controls[\"ID\"] == 1]\n",
    "# y = pd.Series(active_controls[\"ADHD\"], index=active_controls[\"ID\"])\n",
    "# y = y.fillna(0)\n",
    "# y[\"ADHD\"] = active_controls[\"ADHD\"]\n",
    "# y.index = active_controls[\"ID\"]\n",
    "# y[\"ID\"] = y.index\n",
    "# print(y)\n",
    "# print(list(y.index))\n",
    "# print(demographics_01)\n",
    "# print(activity_data_01)\n",
    "# print(y.loc[1:3])\n",
    "\n",
    "# activity_data_01.plot(figsize=(10,10))\n",
    "# plt.show()\n",
    "\n",
    "# Find relevant features using tsfresh\n",
    "# relevant_features = extract_relevant_features(mega_activity_df[mega_activity_df[\"ID\"] <= 3], y.loc[1:3], column_id='ID', column_sort='TIMESTAMP')\n",
    "\n",
    "# relevant_features.head()\n",
    "\n",
    "# X_full_train, X_full_test, y_train, y_test = train_test_split(relevant_features, y, test_size=.3, random_state=42)\n",
    "# X_filtered_train = select_features(X_full_train, y_train)\n",
    "# X_filtered_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting into training set and testing set\n",
    "\n",
    "# # Don't let the machine know which ones are ADHD and which ones aren't\n",
    "# X = controls_info.drop(\"ADHD\", axis=1)\n",
    "# y = controls_info[\"ADHD\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify = y, random_state=2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
